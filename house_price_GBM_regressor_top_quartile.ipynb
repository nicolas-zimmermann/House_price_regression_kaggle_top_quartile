{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b687b2a-d8d2-4143-ad14-d0fb7ca56fdb",
   "metadata": {},
   "source": [
    "# House prices advanced regression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8058f1dc-0333-4590-b9a6-314eb945e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, zscore\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "cc66bc3a-97f3-4ad1-aca5-9a4b8b904d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan_dummy(series):\n",
    "    \"\"\"Given a Series containing NaN and several classes return a dummy Series\n",
    "    indicating 0 for NaN and 1 for non-NaN data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - series : pd.Series, input series or col to dummify\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    - s : pd.Series the dummy Series\n",
    "    \"\"\"\n",
    "    s = series.notna().astype(int).astype(\"category\")\n",
    "    s.name = f\"{series.name}_abs_pres\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9495bcb9-178f-4e2a-b662-0970cbd4a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./train.csv\")\n",
    "\n",
    "# dropping the Id column which isn't informative\n",
    "data = data.drop(\"Id\", axis=1)\n",
    "\n",
    "# Put SalePrice as first column\n",
    "new_cols = data.columns.to_list()\n",
    "new_cols.reverse()\n",
    "data = data[new_cols]\n",
    "\n",
    "\n",
    "# identifying variables with NaN\n",
    "all_nan = data.isnull().sum().sort_values(ascending=False)\n",
    "all_nan = all_nan[all_nan > 0]\n",
    "\n",
    "# looking at the content of all_nan, we can assume we need the following variables\n",
    "# to have a presence/absence aditional dummy variable\n",
    "# Garage, Basement, Pool, Alley, Fence, Fireplace, Front lot, Masonry veneer\n",
    "cols = [\"PoolQC\", \"Alley\", \"Fence\", \"FireplaceQu\", \"LotFrontage\", \"MasVnrArea\",\n",
    "        \"GarageQual\", \"BsmtQual\"]\n",
    "abs_pres_series = []\n",
    "\n",
    "for col in cols:\n",
    "    abs_pres_series.append(get_nan_dummy(data[col]))\n",
    "    \n",
    "abs_pres_series = pd.concat(abs_pres_series, axis=1)\n",
    "data = pd.concat([data, abs_pres_series], axis=1) # we choose to either add these columns or not later\n",
    "\n",
    "# There is a difference of NaN for Bsmt variables, let's look closer\n",
    "a = data[\"BsmtExposure\"].isna()[data[\"BsmtExposure\"].isna() == True].index # 38 NaN\n",
    "b = data[\"BsmtCond\"].isna()[data[\"BsmtCond\"].isna() == True].index # 37 NaN\n",
    "mystery_row = [x for x in (set(a) - set(b))] \n",
    "mystery_row = mystery_row[0] # because there is only 1 element\n",
    "data.loc[mystery_row,[\"BsmtExposure\", \"BsmtFinType2\", \"BsmtCond\", \"BsmtQual\"]]\n",
    "\n",
    "# we can either drop the data or assume BsmtExposure=No bc unfinished\n",
    "# lets drop it\n",
    "data.drop(mystery_row, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "08b596ff-b79c-4bb7-b9c4-13c2e42d6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will fill the NaN of each values accordingly\n",
    "# for qualitative data, Na suggest an absence rather than a lake of observation\n",
    "#    so we assume that absence is actually an observation for some variables\n",
    "#    we name them \"abs\" for abscence\n",
    "data[\"PoolQC\"].fillna(\"abs\", inplace=True)\n",
    "data[\"MiscFeature\"].fillna(\"abs\", inplace=True)\n",
    "data[\"Alley\"].fillna(\"abs\", inplace=True)\n",
    "data[\"Fence\"].fillna(\"abs\", inplace=True)\n",
    "data[\"FireplaceQu\"].fillna(\"abs\", inplace=True)\n",
    "data[\"MasVnrType\"].fillna(\"abs\", inplace=True)\n",
    "data[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "data[\"Electrical\"].fillna(\"abs\", inplace=True)\n",
    "\n",
    "#data[\"LotFrontage\"] ? since there is no value of 0 in the column\n",
    "# we can assume NaN -> 0\n",
    "data[\"LotFrontage\"].fillna(0, inplace=True)\n",
    "\n",
    "# We could drop the NaN Bsmt and Garage rows, we leave it as an option for the time being only keeping the indexs\n",
    "#data.dropna(subset=[\"GarageQual\", \"BsmtExposure\"], inplace=True)\n",
    "na_indexes = np.unique(np.where(data[['GarageQual', 'BsmtExposure']].isna() == True)[0]).tolist()\n",
    "\n",
    "garage_mean = data[\"GarageYrBlt\"].mean()\n",
    "data[\"GarageYrBlt\"].fillna(garage_mean, inplace=True)\n",
    "data[\"GarageCond\"].fillna(\"abs\", inplace=True)\n",
    "data[\"GarageType\"].fillna(\"abs\", inplace=True)\n",
    "data[\"GarageFinish\"].fillna(\"abs\", inplace=True)\n",
    "data[\"GarageQual\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtExposure\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtFinType2\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtCond\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtQual\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtFinType1\"].fillna(\"abs\", inplace=True)\n",
    "# for MasVnrArea, MasVnrType and Electrical, considering the few NA we choose\n",
    "#    to drop it\n",
    "data.dropna(subset=[\"MasVnrArea\", \"MasVnrType\", \"Electrical\"], inplace=True)\n",
    "\n",
    "# Changing CentralAir to one-hot\n",
    "data[\"CentralAir\"] = data[\"CentralAir\"].replace({'Y': 1, 'N': 0}).astype(\"uint8\")\n",
    "\n",
    "# Let's look at the numerical data\n",
    "data.select_dtypes(include=np.number)\n",
    "\n",
    "# we have to change MSSubClass to categories\n",
    "data[\"MSSubClass\"] = data[\"MSSubClass\"].astype(\"category\")\n",
    "\n",
    "# we now have 36 numerical variables, let's select them\n",
    "cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# selecting the continuous variables, candidate for log transform\n",
    "continuous = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\",\n",
    "              \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\",\n",
    "              \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\",\n",
    "              \"WoodDeckSF\", \"OpenPorchSF\", \"PoolArea\", \"SalePrice\"]\n",
    "\n",
    "# Doing the log transform, to do so we switch 0 -> NaN then from Nan -> 0 back again\n",
    "data[continuous] = np.log(data[continuous].replace(0, np.nan)) # log transform, switching to nan to ingore the 0\n",
    "data[continuous] = data[continuous].replace(np.nan, 0) # replacing nan by 0 again\n",
    "\n",
    "# select categorical data\n",
    "categoricals = data.select_dtypes(include=\"object\").columns.to_list()\n",
    "\n",
    "data[categoricals] = data[categoricals].astype(\"category\")\n",
    "enc = OrdinalEncoder()\n",
    "data[categoricals] = pd.DataFrame(enc.fit_transform(data[categoricals]),\n",
    "                                   columns=categoricals, index=data.index)\n",
    "\n",
    "#data.to_csv(\"./clean_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "35119486-7e39-40bc-b8b1-dec64ef551e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 88)\n",
      "(1409, 88)\n"
     ]
    }
   ],
   "source": [
    "# Outlayers selection and dropping\n",
    "## calculating the z-scores for values than are non zeros i.e excluding missing data\n",
    "data[continuous] = data[continuous].replace(0, np.nan)\n",
    "z_scores = []\n",
    "for col in continuous:\n",
    "    z_scores.append(zscore(data[continuous][col].dropna()).sort_values(ascending=False))\n",
    "    \n",
    "data[continuous] = data[continuous].replace(np.nan, 0) # setting back again NaN -> 0\n",
    "\n",
    "## selecting outlayers indexes\n",
    "threshold = 3.50 # z score limits\n",
    "outlayers_list = []\n",
    "\n",
    "for i in range(len(z_scores)):\n",
    "    outlayers_list.extend(z_scores[i][z_scores[i] > threshold].index.tolist()) # select all outlayers > z\n",
    "    outlayers_list.extend(z_scores[i][z_scores[i] < -threshold].index.tolist()) # select all outlayers < -z\n",
    "\n",
    "outlayers_list = [x for x in set(outlayers_list)] # remove multiple occurences\n",
    "\n",
    "## Dropping outlayers\n",
    "print(data.shape)\n",
    "data.drop(inplace=True, index=outlayers_list)\n",
    "print(data.shape)\n",
    "\n",
    "# Saving data without outlayers\n",
    "#data.to_csv(\"./clean_train.csv\", index=False)\n",
    "## Dropping houses with no basements or no Garage\n",
    "#data.drop(na_indexes, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "31af6869-ca02-4d1a-806f-f3947863eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "eea2082b-08b4-4b2c-89ba-f51a1b5c14f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting input and target variables\n",
    "X = data.iloc[:,1:]\n",
    "y = data.iloc[:,0]\n",
    "\n",
    "# shuffled data for cross val\n",
    "X_shuf, y_shuf = shuffle(X, y, random_state=0)\n",
    "\n",
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "71f47c96-25da-423d-a971-e81855bde00f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # making a pipepline with ordinal encoder\n",
    "# ordinal_encoder = make_column_transformer(\n",
    "#     (\n",
    "#         OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan),\n",
    "#         make_column_selector(dtype_include=\"category\"),\n",
    "#     ),\n",
    "#     remainder=\"passthrough\",\n",
    "#     # Use short feature names to make it easier to specify the categorical\n",
    "#     # variables in the HistGradientBoostingRegressor in the next step\n",
    "#     # of the pipeline.\n",
    "#     verbose_feature_names_out=False,\n",
    "# )\n",
    "\n",
    "# reg_pipe = make_pipeline(\n",
    "#     ordinal_encoder,\n",
    "#     HistGradientBoostingRegressor(\n",
    "#         random_state=0,\n",
    "#         categorical_features=categoricals,\n",
    "#     ),\n",
    "# ).set_output(transform=\"pandas\")\n",
    "\n",
    "# #testing the pipeline on a 5fold CV\n",
    "# #results = cross_validate(reg_pipe, X, y, cv=5)\n",
    "\n",
    "# # Hyperparameter tunning\n",
    "\n",
    "# param_grid = {\"histgradientboostingregressor__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.4, 0.7, 1],\n",
    "#                \"histgradientboostingregressor__max_iter\": [100, 200, 400, 600, 1000, 1400, 1800, 2500],\n",
    "#                \"histgradientboostingregressor__max_leaf_nodes\": [2, 4, 6, 10, 20, 40, 100, None],\n",
    "#                \"histgradientboostingregressor__max_depth\": [1, 2, 5, 10, 20, 30, 40, 50, None],\n",
    "#                \"histgradientboostingregressor__min_samples_leaf\": [1, 2, 3, 4, 5, 8, 10, 20]}\n",
    "\n",
    "# search = HalvingRandomSearchCV(reg_pipe, param_distributions=param_grid,\n",
    "#                                max_resources=10,\n",
    "#                                random_state=0).fit(X, y)\n",
    "\n",
    "# optimal_d = {}\n",
    "# #removing 'histgradientboostingregressor__' in front of param\n",
    "# rem = len('histgradientboostingregressor__')\n",
    "# for (key, value) in search.best_params_.items():\n",
    "#     optimal_d[key[rem:]] = value\n",
    "\n",
    "# optimal_d = {'min_samples_leaf': 5,\n",
    "#  'max_leaf_nodes': 40,\n",
    "#  'max_iter': 400,\n",
    "#  'max_depth': 30,\n",
    "#  'learning_rate': 0.01,\n",
    "#  'categorical_features': categoricals}\n",
    "    \n",
    "# reg_pipe_opt = make_pipeline(\n",
    "#     ordinal_encoder,\n",
    "#     HistGradientBoostingRegressor(**optimal_d),\n",
    "# ).set_output(transform=\"pandas\")\n",
    "\n",
    "# results = cross_validate(reg_pipe_opt, X, y, cv=5)\n",
    "# results['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "70f8ce2a-89ea-4e06-a128-d653c600b580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reg_pipe = make_pipeline(\n",
    "#     ordinal_encoder,\n",
    "#     HistGradientBoostingRegressor(\n",
    "#         random_state=0,\n",
    "#         categorical_features=categoricals,\n",
    "#     ),\n",
    "# ).set_output(transform=\"pandas\")\n",
    "\n",
    "# results = cross_validate(reg_pipe, X, y, cv=5)\n",
    "# results['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "0ce84a3b-1e98-4da2-ab62-9b51cf25c2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.36%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=10, max_leaf_nodes=2, min_samples_leaf=15,\n",
       "                          min_samples_split=4, n_estimators=1600)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=10, max_leaf_nodes=2, min_samples_leaf=15,\n",
       "                          min_samples_split=4, n_estimators=1600)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=10, max_leaf_nodes=2, min_samples_leaf=15,\n",
       "                          min_samples_split=4, n_estimators=1600)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = GradientBoostingRegressor()\n",
    "\n",
    "# random_grid = {'n_estimators' : [100, 200, 400, 600, 800, 1200, 1600],\n",
    "#                'learning_rate' : [0.01, 0.1, 0.2, 0.4, 0.6, 1, 1.5, 2],\n",
    "#                'min_samples_split' : [2, 3, 4, 6, 10],\n",
    "#                'min_samples_leaf' : [1, 2, 3, 4, 5, 8, 10],\n",
    "#                'max_depth' : [1, 2, 5, 10, 20, 30, 40, 50, 80, 100],\n",
    "#                'min_samples_split' : [2, 4, 8],\n",
    "#                'min_samples_leaf' : [1, 2, 4, 6, 8, 10, 15],\n",
    "#                'max_leaf_nodes' : [2, 4, 6, 10, 20, 40],}\n",
    "\n",
    "#search = RandomizedSearchCV(estimator = reg, param_distributions = random_grid,\n",
    "#                               n_iter = 200, cv = 3, verbose=1, random_state=0, n_jobs = -1)\n",
    "\n",
    "#search.fit(X,y)\n",
    "\n",
    "#saving best hyperparameter set as a dict\n",
    "#best_d = search.best_params_\n",
    "\n",
    "best_d = {'n_estimators': 1600,\n",
    "          'min_samples_split': 4,\n",
    "          'min_samples_leaf': 15,\n",
    "          'max_leaf_nodes': 2,\n",
    "          'max_depth': 10,\n",
    "          'learning_rate': 0.1}\n",
    "\n",
    "reg = GradientBoostingRegressor(**best_d)\n",
    "\n",
    "score = cross_val_score(\n",
    "             reg, X_shuf, y_shuf, cv=5)\n",
    "\n",
    "print(f\"{score.mean():.2%}\")\n",
    "\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eec4ac-bdd3-4a3e-848d-c7a6d14850de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with z thres = 3.6 and no Na deletion : 90.10% mean CV score\n",
    "# with z thres = 3.6 and Na deletion : 90.10% mean CV score\n",
    "\n",
    "# with z thres = 4 and no Na deletion : 89.62% mean CV score\n",
    "# with z thres = 4 and Na deletion : 89.62% mean CV score\n",
    "\n",
    "# with z thres = 3.4 and no Na deletion : 91.03% mean CV score\n",
    "# with z thres = 3.4 and no Na deletion and n_estimators=1800 : 91.06% mean CV score\n",
    "# with z thres = 3.4 and Na deletion and n_estimators=1800 : 91.06% mean CV score\n",
    "\n",
    "# with z thres = 3.5 and no Na deletion and n_estimators=1800 : 91.28% mean CV score\n",
    "\n",
    "# with z thres = 3.2 and no Na deletion and n_estimators=1800 : 90.84% mean CV score\n",
    "\n",
    "# with z thres = 3.5 no Na deletion, best param and NO abs/pres columns :  90.80 %\n",
    "# with z thres = 3.5 no Na deletion, best param and abs/pres columns : 91.21%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81914199-34c8-479b-b61d-5489b6cede06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note for later : maybe remove outlayers after a test/train split set to evaluate\n",
    "# the impact of extreme value drop on generalisation abilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e033ce-0006-4fb1-9182-fc635666feca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Outlayers selection and dropping\n",
    "## calculating the z-scores for values than are non zeros i.e excluding missing data\n",
    "X_train[continuous] = X_train[continuous].replace(0, np.nan)\n",
    "z_scores = []\n",
    "for col in continuous:\n",
    "    z_scores.append(zscore(X_train[continuous][col].dropna()).sort_values(ascending=False))\n",
    "    \n",
    "X_train[continuous] = X_train[continuous].replace(np.nan, 0) # setting back again NaN -> 0\n",
    "\n",
    "## selecting outlayers indexes\n",
    "threshold = 3.55 # z score limits\n",
    "outlayers_list = []\n",
    "\n",
    "for i in range(len(z_scores)):\n",
    "    outlayers_list.extend(z_scores[i][z_scores[i] > threshold].index.tolist()) # select all outlayers > z\n",
    "    outlayers_list.extend(z_scores[i][z_scores[i] < -threshold].index.tolist()) # select all outlayers < -z\n",
    "\n",
    "outlayers_list = [x for x in set(outlayers_list)] # remove multiple occurences\n",
    "\n",
    "## Dropping outlayers\n",
    "print(X_train.shape)\n",
    "X_train.drop(inplace=True, index=outlayers_list)\n",
    "y_train.drop(inplace=True, index=outlayers_list)\n",
    "print(X_train.shape)\n",
    "\n",
    "## Dropping houses with no basements or no Garage\n",
    "#data.drop(na_indexes, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831b6b3-32dd-4b3f-819c-052609c1b3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# dropping the Id column which isn't informative\n",
    "data = data.drop(\"Id\", axis=1)\n",
    "\n",
    "# Put SalePrice as first column\n",
    "new_cols = data.columns.to_list()\n",
    "new_cols.reverse()\n",
    "data = data[new_cols]\n",
    "\n",
    "# looking at the content of all_nan, we can assume we need the following variables\n",
    "# to have a presence/absence aditional dummy variable\n",
    "# Garage, Basement, Pool, Alley, Fence, Fireplace, Front lot, Masonry veneer\n",
    "cols = [\"PoolQC\", \"Alley\", \"Fence\", \"FireplaceQu\", \"LotFrontage\", \"MasVnrArea\",\n",
    "        \"GarageQual\", \"BsmtQual\"]\n",
    "abs_pres_series = []\n",
    "\n",
    "for col in cols:\n",
    "    abs_pres_series.append(get_nan_dummy(data[col]))\n",
    "    \n",
    "abs_pres_series = pd.concat(abs_pres_series, axis=1)\n",
    "data = pd.concat([data, abs_pres_series], axis=1) # we choose to either add these columns or not later\n",
    "\n",
    "# now we will fill the NaN of each values accordingly\n",
    "# for qualitative data, Na suggest an absence rather than a lake of observation\n",
    "#    so we assume that absence is actually an observation for some variables\n",
    "#    we name them \"abs\" for abscence\n",
    "data[\"PoolQC\"].fillna(\"abs\", inplace=True)\n",
    "data[\"MiscFeature\"].fillna(\"abs\", inplace=True)\n",
    "data[\"Alley\"].fillna(\"abs\", inplace=True)\n",
    "data[\"Fence\"].fillna(\"abs\", inplace=True)\n",
    "data[\"FireplaceQu\"].fillna(\"abs\", inplace=True)\n",
    "data[\"MasVnrType\"].fillna(\"abs\", inplace=True)\n",
    "data[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "data[\"Electrical\"].fillna(\"abs\", inplace=True)\n",
    "\n",
    "#data[\"LotFrontage\"] ? since there is no value of 0 in the column\n",
    "# we can assume NaN -> 0\n",
    "data[\"LotFrontage\"].fillna(0, inplace=True)\n",
    "\n",
    "# We could drop the NaN Bsmt and Garage rows, we leave it as an option for the time being only keeping the indexs\n",
    "#data.dropna(subset=[\"GarageQual\", \"BsmtExposure\"], inplace=True)\n",
    "na_indexes = np.unique(np.where(data[['GarageQual', 'BsmtExposure']].isna() == True)[0]).tolist()\n",
    "\n",
    "garage_mean = data[\"GarageYrBlt\"].mean()\n",
    "data[\"GarageYrBlt\"].fillna(garage_mean, inplace=True)\n",
    "data[\"GarageCond\"].fillna(\"abs\", inplace=True)\n",
    "data[\"GarageType\"].fillna(\"abs\", inplace=True)\n",
    "data[\"GarageFinish\"].fillna(\"abs\", inplace=True)\n",
    "data[\"GarageQual\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtExposure\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtFinType2\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtCond\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtQual\"].fillna(\"abs\", inplace=True)\n",
    "data[\"BsmtFinType1\"].fillna(\"abs\", inplace=True)\n",
    "# for MasVnrArea, MasVnrType and Electrical, considering the few NA we choose\n",
    "#    to drop it\n",
    "#data.dropna(subset=[\"MasVnrArea\", \"MasVnrType\", \"Electrical\"], inplace=True)\n",
    "\n",
    "# Changing CentralAir to one-hot\n",
    "data[\"CentralAir\"] = data[\"CentralAir\"].replace({'Y': 1, 'N': 0}).astype(\"uint8\")\n",
    "\n",
    "# Let's look at the numerical data\n",
    "data.select_dtypes(include=np.number)\n",
    "\n",
    "# we have to change MSSubClass to categories\n",
    "data[\"MSSubClass\"] = data[\"MSSubClass\"].astype(\"category\")\n",
    "\n",
    "# we now have 36 numerical variables, let's select them\n",
    "cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# selecting the continuous variables, candidate for log transform\n",
    "continuous = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\",\n",
    "              \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\",\n",
    "              \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\",\n",
    "              \"WoodDeckSF\", \"OpenPorchSF\", \"PoolArea\"]\n",
    "\n",
    "# Doing the log transform, to do so we switch 0 -> NaN then from Nan -> 0 back again\n",
    "data[continuous] = np.log(data[continuous].replace(0, np.nan)) # log transform, switching to nan to ingore the 0\n",
    "data[continuous] = data[continuous].replace(np.nan, 0) # replacing nan by 0 again\n",
    "\n",
    "# select categorical data\n",
    "categoricals = data.select_dtypes(include=\"object\").columns.to_list()\n",
    "\n",
    "data[categoricals] = data[categoricals].astype(\"category\")\n",
    "enc = OrdinalEncoder()\n",
    "data[categoricals] = pd.DataFrame(enc.fit_transform(data[categoricals]),\n",
    "                                   columns=categoricals, index=data.index)\n",
    "\n",
    "data.to_csv(\"./clean_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9166f5-6028-44c2-88d3-b94b2534f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"GarageQual_abs_pres\"][1116] # indicates there is no garage so GarageCars = 0\n",
    "data.loc[1116,\"GarageCars\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "77a29755-a497-4484-afd9-43719a9b91fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the other data we are going to create a serie of classifiers to infer the missing informations\n",
    "# Since Saleprice is missing for the rows in which we try to infer variables\n",
    "# we will combine the train and test data, removing SalePrice from the train data, in order to have\n",
    "# more observation to train our classifiers\n",
    "df_train = pd.read_csv(\"./clean_train.csv\")\n",
    "df_train.drop(\"SalePrice\", axis=1, inplace=True)\n",
    "df_test = pd.read_csv(\"./clean_test.csv\")\n",
    "\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis = 0, ignore_index=True)\n",
    "df_train = df.dropna() # dropping NaNs which are the also rows to infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820405e-0375-4b90-894b-993f54083031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are missing values in the data, we need to take a closer look\n",
    "all_nan = df_test.isna().sum().sort_values(ascending=False)\n",
    "all_nan = all_nan[all_nan > 0]\n",
    "\n",
    "# Now lets get all the indexes of values to predict\n",
    "to_predict = {}\n",
    "for col in all_nan.index.to_list():\n",
    "    to_predict[col] = (df_test[col][df_test[col].isna() == True]).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c124b-e550-466f-8d0a-dd53385e5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(data, col):\n",
    "    \"\"\"Build a classifier for the variable specified in col\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "     - data, pd.Dataframe : The data used to train the classifier\n",
    "     - col, string : Column name of the variable to infer, must be in data\n",
    "     \n",
    "    Returns\n",
    "    -------\n",
    "     - clf, HistGradientBoostingClassifier() : trained classifier\n",
    "     \"\"\"   \n",
    "    X = data.loc[:, data.columns != col]\n",
    "    y = data[col]\n",
    "\n",
    "    clf = HistGradientBoostingClassifier()\n",
    "\n",
    "    random_grid = {\"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.4, 0.7, 1],\n",
    "                   \"max_iter\": [100, 200, 400, 600, 1000, 1400, 1800, 2500],\n",
    "                   \"max_leaf_nodes\": [2, 4, 6, 10, 20, 40, 100],\n",
    "                   \"max_depth\": [1, 2, 5, 10, 20, 30, 40, 50],\n",
    "                   \"min_samples_leaf\": [1, 2, 3, 4, 5, 8, 10, 20]}\n",
    "\n",
    "    search = RandomizedSearchCV(estimator = clf, param_distributions = random_grid,\n",
    "                                  n_iter = 100, cv = 3, verbose=1, random_state=0, n_jobs = -1)\n",
    "    search.fit(X,y)\n",
    "    best_d = search.best_params_\n",
    "    \n",
    "    clf = HistGradientBoostingClassifier(**best_d)\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10037771-f71c-4a02-add9-e943d8eb4b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filled NA with predictors\n",
    "# for col in to_predict:\n",
    "#     clf = build_classifier(df_train, col)\n",
    "#     print(col)\n",
    "#     print(df_test.loc[to_predict[col], col])\n",
    "#     df_test.loc[to_predict[col], col] = clf.predict(df_test.drop(col, axis=1).loc[to_predict[col],:])\n",
    "#     print(df_test.loc[to_predict[col], col])\n",
    "\n",
    "# df_test.to_csv(\"./clean_nona_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "19a62460-71a7-43d7-a957-ec183f1b40fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_test = pd.read_csv(\"./clean_nona_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3fd3f7c0-4a4c-4da5-a4e9-e2b42890f675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_test[categoricals] = df_test[categoricals].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "ddffbbc1-4f4b-427d-949f-8f2284c4ef5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y_pred = reg.predict(df_test)\n",
    "#y_pred = pd.Series(np.exp(y_pred), name=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e8d906ea-da93-45b4-9c6c-2c1ba8f30c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#original_df = pd.read_csv(\"./test.csv\")\n",
    "#res = pd.concat([original_df[\"Id\"], y_pred], names=[\"Id\", \"SalePrice\"], axis=1)\n",
    "#res.to_csv(\"resultsGBM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea805942-d739-46c3-b7ec-1ff3f2315f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./clean_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e865-e07a-4e0c-b711-8e2897b08b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
